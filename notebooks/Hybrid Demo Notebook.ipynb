{"nbformat_minor": 0, "cells": [{"execution_count": 1, "cell_type": "code", "source": "sc.addPyFile('/local/path/to/sb/soft-boiled.zip')\nfrom src.algorithms import slp, gmm\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "# Raw Data Sources", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data_path = 'hdfs:///post_etl_datasets/twitter/year=2015'\nall_tweets = sqlCtx.read.parquet(data_path)\nall_tweets.registerTempTable('all_tweets')", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "# Use Pre-trained GMM model, filter \"poor\" points", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "gmm_model = gmm.load_model('/local/path/to/gmm/model.csv.gz')", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "gmm_model_filtered = {}\n\npercentile = 25\nerror_at_Nth_percentile = np.percentile([gmm_model[word][1] for word in gmm_model],percentile)\nprint 'Error (km) at Nth percentile:', error_at_Nth_percentile\n\nfor word in gmm_model:\n    if gmm_model[word][1]<=error_at_Nth_percentile:\n        gmm_model_filtered[word] = gmm_model[word]", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# Load Saved SLP Data", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "holdout_10pct = lambda (src_id) : src_id[-1] != '9'\n# Previously computed known User Locations -- result of slp.get_known_locs\n# Note: Dispersion threshold should be set very low and filter later in hyper parameter search\nlocs_known = sc.pickleFile('hdfs:///path/to/slp/get_known_locs').cache()\n\n# Previously computed edge list -- result of slp.get_edge_list\nedge_list = sc.pickleFile('hdfs:///path/to/slp/get_edge_list').cache()", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Map side join by hand -- avoids having to shuffle all the data ever... \n#     This is an annoying, probably necessary optimization\n# Locatable Ids are ids in the edge list. Since edges are bi-directional (so src-> dst and dst-> src are both in edge list) \n# we can just keep distinct(src)\nlocatable_ids_local = edge_list.map(lambda (src, (dst,weight)): src).distinct().collect()\nbroadcast_set = sc.broadcast(set(locatable_ids_local))\njune_tweets = all_tweets.rdd.filter(lambda row: row != None and row.user!=None and row.user.id_str in broadcast_set.value)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Use predict user function to estimate user locations of \"locatable_ids\"\n# Note: This is very time consuming so save results to allow us to do further work on \nimport datetime\nimport itertools\nstart_time = datetime.datetime.now()\n# Note: Set predict_lower bound to be very low and then filter later in hyper parameter search\ngmm_locations_no_filter2 = gmm.predict_user_gmm(sc, june_tweets,['user.location', 'text'], gmm_model_filtered, \\\n                                                radius=100, predict_lower_bound=.0001)\ngmm_locations_no_filter2.saveAsPickleFile('hdfs:///path/to/save/gmm/predict_user')\nelapsed_time = datetime.datetime.now() - start_time\nprint elapsed_time", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "# Get known locations and broadcast for filter below\nlocs_known_set_bcast = sc.broadcast(set(locs_known.map(lambda (id_str, loc_est): id_str).collect()))\n\n# Load saved results from above (even if we just calculated it) to force repartition \ngmm_locations_no_filter = sc.pickleFile('hdfs:///path/to/save/gmm/predict_user').coalesce(400)\n\n# Filter out locations that are already in locs known\ngmm_locations_no_filter_loc_est = gmm_locations_no_filter\\\n    .filter(lambda (id_str, loc_est): id_str not in locs_known_set_bcast.value)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# Run Single Test of Hybrid", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "known_threshold = 150\ndispersion_threshold = 250\ngmm_percent_threshold = 0\nnum_iters = 6\n\n# Filter known locs to only keep known locs below some dispersion threshold\ndispersion_filtered_locs = locs_known\\\n                .filter(lambda (id_str, loc_estimate): loc_estimate.dispersion < known_threshold)\n\n# If you need to filter based on percent chance that position is within some radius\ngmm_locations_filtered_loc_est = gmm_locations_no_filter_loc_est\\\n                    .filter(lambda (id_str, loc_est): loc_est.dispersion >gmm_percent_threshold)\n\n# Combine locations from gmm.predict_user and slp.get_known_locs\nunioned_locs_known  = dispersion_filtered_locs.union(gmm_locations_filtered_loc_est)\n\n# Apply holdout function\nfiltered_locs_known = unioned_locs_known.filter(lambda (id_str, loc_estimate): holdout_10pct(id_str))\n\n# Train SLP \nestimated_locs = slp.train_slp(filtered_locs_known, edge_list, num_iters, dispersion_threshold=dispersion_threshold)\n# Test using both gmm.predict_user locations and slp.get_known_locs\nerrors_all_test_point = slp.run_slp_test(unioned_locs_known, estimated_locs, holdout_10pct)\nerrors_all_test_point['known_threshold'] = known_threshold\nerrors_all_test_point['dispersion_threshold'] = dispersion_threshold\nerrors_all_test_point['num_iters'] = num_iters\nprint errors_all_test_point\n\n# Test using only points from slp.get_known_locs\nerrrors_slp_known_locs_only = slp.run_slp_test(dispersion_filtered_locs, estimated_locs, holdout_10pct)\nerrrors_slp_known_locs_only['known_threshold'] = known_threshold\nerrrors_slp_known_locs_only['dispersion_threshold'] = dispersion_threshold\nerrrors_slp_known_locs_only['num_iters'] = num_iters\nprint errrors_slp_known_locs_only", "outputs": [{"output_type": "stream", "name": "stdout", "text": "{'num_iters': 6, 'known_threshold': 150, 'median': 12.601169254648408, 'dispersion_threshold': 250, 'num_locs': 256260, 'coverage': 0.10734800593147585, 'mean': 844.29700308525787}\n{'num_iters': 6, 'known_threshold': 150, 'median': 10.233307582508733, 'dispersion_threshold': 250, 'num_locs': 81609, 'coverage': 0.10003798600644537, 'mean': 442.57120792538876}\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "# Hyper Parameter Search", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import datetime\nresults_all = []\nresults_geo_only = []\n# Iterate over desired threshold for slp.get_known_locs\nfor known_threshold in [50]:\n    # Get dispersion filtered locations \n    dispersion_filtered_locs = locs_known\\\n                .filter(lambda (id_str, loc_estimate): loc_estimate.dispersion < known_threshold)\n    # Iterate over dispersion threshold in slp.train\n    for dispersion_threshold in [50, 150, 250]:\n        # Iterate over gmm confidence gmm (GMMLocEstimate.prob)\n        for gmm_percent in [.1,.3, .5, .7, .85, .9, .95]:\n            # Union with GMM locations when available\n            gmm_locations_filtered_loc_est = gmm_locations_no_filter_loc_est\\\n                    .filter(lambda (id_str, loc_est): loc_est.dispersion >gmm_percent)\n            unioned_locs_known  = dispersion_filtered_locs.union(gmm_locations_filtered_loc_est)         \n            filtered_locs_known = unioned_locs_known.filter(lambda (id_str, loc_estimate): holdout_10pct(id_str))\n            # Test accuracy over number of SLP iterations\n            for num_iters in [1, 3, 5, 7, 9]:\n                print datetime.datetime.now(), known_threshold, dispersion_threshold, gmm_percent, num_iters\n                estimated_locs = train(filtered_locs_known, edge_list, num_iters, dispersion_threshold=dispersion_threshold)\n                errors_local = run_test(unioned_locs_known, estimated_locs, holdout_10pct)\n                errors_local['known_threshold'] = known_threshold\n                errors_local['dispersion_threshold'] = dispersion_threshold\n                errors_local['gmm_percent'] = gmm_percent\n                errors_local['num_iters'] = num_iters\n                results_all.append(errors_local)\n                \n                errors_local = run_test(dispersion_filtered_locs, estimated_locs, holdout_10pct)\n                errors_local['known_threshold'] = known_threshold\n                errors_local['dispersion_threshold'] = dispersion_threshold\n                errors_local['gmm_percent'] = gmm_percent\n                errors_local['num_iters'] = num_iters\n                results_geo_only.append(errors_local)\n\nprint 'All Results: ', results_all\nprint 'Results Geo Only: ', results_geo_only\n# Might want to save results since they take a while to compute", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.10", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}